{"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{"cell_id":"00000-6ac93d33-7180-4000-8c7f-2b27aef6eacd","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Every science and engineering discipline relies on differentiation in some capacity, whether seeking to optimize system operations, deriving rates of change, or evaluating complex expressions. In this era of abundant computationally intensive tasks, evaluating gradients of any function (regardless of form) is both practical and valuable. The FADiff package addresses this task by automatically differentiating functions in either forward or reverse mode. By implementing automatic differentiation (AD), which sequentially evaluates elementary functions, FADiff avoids the complexity of symbolic differentiation and the precision issues of numerical differentiation. Additional information on implementation is below.","metadata":{"cell_id":"00001-ed0a2f06-a5ae-4e02-8b83-0f52f76a92f1","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Background","metadata":{"cell_id":"00002-23402e9e-6c54-4fd5-8a52-e3294fe77c87","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Automatic Differentiation (AD) is a set of techniques for evaluating derivatives precisely based on computation graphs, chain rules and other symbolic rules. Compared with manual calculation or a symbolic approach to calculating derivatives, it is highly convenient and fast since it frees users from tedius calculations and proofs. Compared with finite approximation (a.k.a numerical differentiation), it is more accurate in that it avoids truncation errors or round-off errors that might arise in symbolic differentiation when selecting a huge step (h) or a tiny step (h). (We've analyzed this point in HW4.). Due to these advantages, it has been widely used in scientific computing, machine learning, deep learning, etc. \n\nThe mathematical background knowledge mainly includes matrix-vector products, the Jacobian matrix, the algegra of dual numbers, Taylor series expansions, higher-order derivatives, etc. We will discuss them in more details later. There are 2 evaluation modes in AD, **forward mode** and **reverse mode**.\n\n1. Forward mode performs the operation of evaluating the numerical derivative concurrently with evaluating the function itself on a computational graph.\n\n2. Reverse mode is an alternative to the forward mode. It uses the computation graph in forward mode to calculate the final output and then traveres reversely to perform the operation of evaluating derivatives. This mode is commonly used in deep learning and neural networks, in which it is also called backpropogation. \n\n\n","metadata":{"cell_id":"00003-6a571975-82fb-4224-a696-3d6ed4f631a8","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 1. Matrix-vector Products\n##### 1.1 Definition \n   Given an $m\\times n$  matrix $A_{m\\times n}$ and a vector $x\\in R^{n}$, there is a way to create a linear combination\n   \n$$\nx_1a_1 + x_2a_2 + ... + x_na_n \\in R^m \n$$\n\nusing the columns $a_1, . . . , a_n$ of $A$, where $x=\\left[x_1,x_2,...,x_n \\right]^{T}$.\n\n##### 1.2 Notes\n1. Matrix-vector products are only valid when the sizes of the matrix and vector are compatible – the number of elements of vector $x$ must equal the number of columns of matrix $A$. The number of elements in the output vector must equal the number of rows in matrix $A$.\n2. We can interpret matrix-vector products as creating a linear transformation or a **map** from $R^n$ to $R^m$","metadata":{"cell_id":"00004-5db04178-d588-410c-8783-67c6ad616879","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 2. Two Evaluation Modes: Forward & Reverse, Jacobian Matrix.\nAutomatic Differentiation (AD) can be applied on both scalar functions with one variable or functions with multiple variables. The derivative calculation of a single variable is relatively straightforward, while in situations with multiple variables, we will introduce a terminology called the Jacobian Matrix ($J$).\n\n\nLet's start from a general case with $x$ is a vector.","metadata":{"cell_id":"00005-c19b5860-9b26-4150-b5fc-b37d5a98eaae","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##### 2.1 Jacobian Matrix\nIf $f$ is a matrix of multiple functions with multiple input variables, then denote $f$ as \n\n$$\nf=\\begin{bmatrix} f_1(x,y) \\\\ f_2(x,y) \\end{bmatrix}\n$$\n\nThen, the derivative of matrix $f$ is called the Jacobian Matrix $J$:\n\n$$\n\\begin{aligned}\n  J = \n  \\begin{bmatrix}\n    \\partial f_{1} / \\partial x & \\partial f_{1} / \\partial y \\\\\n    \\partial f_{2} / \\partial x & \\partial f_{2} / \\partial y\n  \\end{bmatrix}\n\\end{aligned}\n$$","metadata":{"cell_id":"00006-0d13ad32-d233-4e83-b04c-b702681e735b","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##### 2.2 Forward Mode\nA program can be written as a combination of several functions: $f = f_1 ... f_n$. Let's set $x_0$ as a vector in $R^n$ and $x_n$ as the output vector; then each $f_i$ is the transaction function (a generalized \"matrix\" from the definition of matrix-vector products), and \n\n$$\nx_1 = f_1x_0\n$$\n$$\nx_2 = f_2x_1\n$$\n$$\n...\n$$\n$$\nx_n=f_nx_{n-1}.\n$$\nFrom the chain rule, we have:\n\n$$\n\\dot{x_1} =  (J f_1 x_0)\n$$\n$$\n\\dot{x_2} =  (J f_2 x_1) \\times \\dot{x_1}\n$$\n$$\n ... \n $$\n$$ \n\\dot{x_n} = (J f_n x_{n-1})\\times \\dot{x_{n-1}}. \n$$\n\nThe above process of evaluating derivatives is called **forward mode Automatic Differentiation**.\n\n\n","metadata":{"cell_id":"00007-9d2f9f92-aa61-4684-b53e-c75e364f3a76","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##### 2.3 Reverse Mode\nIf we take the transpose of both the left and right sides of equation (1),(2)...(n) above, then \n\n$${x_1}^\\prime = (f_1x_0)^T$$\n$${x_2}^\\prime = (f_2x_1)^T$$\n$$...$$\n$${x_n}^\\prime = (f_nx_{n-1})^T.$$\n\nFrom the chain rule, we have:\n\n$$ {x_{n-1}}^\\prime =  (J f_n x_{n-1})^T$$\n$$ {x_{n-2}}^\\prime =  (J f_{n-1} x_{n-2}) \\times {x_{n-1}}^\\prime $$\n$$ ... $$\n$$ {x_0}^\\prime = (J f_1 x_0)\\times {x_1}^\\prime. $$\n\nThe above process of evaluating derivatives is called **reverse mode Automatic Differentiation**.\n\n\n","metadata":{"cell_id":"00008-00086732-90a9-495c-a984-3f19577fcfa7","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##### 2.4 Example of computational graph, forward and reverse mode.\n\n![image.png](/image.png)","metadata":{"cell_id":"00009-cf5c46ee-44b2-41c4-bc04-fd810662899c","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##### 2.5 When to use reverse or forward mode?\n\nThe difference between forward and reverse mode lies in the start point of matrix multiplication.\n\nWhen the dimension of the input is less than that of the output, forward mode has fewer multiplication operations than reverse mode; comparably, when the dimension of the input is more than that of the output, reverse mode has fewer multiplication operations. \n\nTherefore, when the dimension of the input is less than that of the output, forward mode is more efficient; when the dimension of the input is more than that of the output, reverse mode is more efficient. ","metadata":{"cell_id":"00010-1ab825af-5c90-4cc1-89de-d18db6fc648e","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 3. The algebra of dual numbers\n##### 3.1 Definition\nA dual number (z) is composed of a real part (a) and a dual part (b).  We denote it as \n\n$$z = a + \\epsilon b.$$\n\n##### 3.3 What's the effect of dual numbers on derivatives? \nThe usage of a dual number augments the arithmetic in real number space to any input and allows the user to get the derivatives without calculating them. A function f(x) where x is a dual number can be re-written in a dual number format, where the real component is the function and the dual component contains the derivative (as we discussed in lecture 10).\n\nGenerally, let $\\hat f$ denote the expansion of real-value function $f$ to dual number space, then\n\n$$\\hat f(x_1+x_1^\\prime\\epsilon, ..., x_n + x_n^\\prime\\epsilon):=f(x_1,...,x_n)+\\dot f(x_1,...x_n) \\begin{pmatrix} x_1^\\prime \\\\ \\vdots \\\\ x_n^\\prime\\end{pmatrix}\\epsilon $$\n\nIf $f$ is a matrix of multiple differentiable functions, then we can extend the above framework by replacing $\\dot f(x_1,...x_n)$ with $J f (x_1,...,x_n)$.\n","metadata":{"cell_id":"00011-359781dc-757d-4501-98ce-46f0bf896ad7","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 4. Elemental functions\n\nAutomatic Differentiation relies on the fact that we already know the derivative at each step. So, we need some elemental functions. A function is called \"elemental\" if it always returns the same result for the same argument values and has no side effect like modifying a global variable, etc. The result of calling an elemental function is the exact return value. \n\nSome examples are pow(), mean(), sqrt(), while printf(), rand() and time() are not elemental functions.\n","metadata":{"cell_id":"00012-e34784ab-8ccf-42bb-9b39-60d417ca1521","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## How to use FADIFF\n","metadata":{"cell_id":"00013-7e1db5fe-fdad-4060-a893-2c835adafd7a","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We expect the use of our package, `FADiff`, to be through its API located in the module `FADiff.py`. The API takes in Python ints, floats, and `FADiff` objects as parameters as well as lists of said objects. Each `FADiff` object has `val` and `der` methods with an @property decorator attached that allow the user to retrieve the outputs of an automatic differentiation calculation. These return values are NumPy arrays and can be indexed into. The elements in `der` are the partial derivatives of the object with respect to the input variables and `val` is the value for the object. For certain areas of our implementation, our package requires the exclusive use of internally defined objects and functions. For example, we prohibit users from using external libraries for elementary functions (e.g., sine and cosine) with our package’s variables and only allow them to use our package’s implementations for such functions. This reduces the potential for issues such as disuse or misuse of our package’s operator-overloaded functions, among other things. How the user should use our package’s API including the proper use of variables and methods will be further explained in the next sections.\n","metadata":{"cell_id":"00015-b2245acc-d1d9-4f25-bace-5aef24e94592","tags":[],"output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Run\n\nOur package can be downloaded from PyPI by running the following in a terminal window:\n\n```\npip install FADiff==0.19\n```\n\nWe implemented our package with Python 3.8.2 on Linux but other versions of Python may still be compatible. In a user’s code file, two headers are needed to import and use our entire package. Shown below is an example demonstration of the use of our package in Python. The following code would be in a user’s code file:\n\n```\nfrom FADiff import FADiff as ad      # User must import   \nfrom FADiff import Elems as ef       # User must import\n\nad.set_mode('forward')           # User MUST set mode of AD at TOP of their code\n                                 #   ('forward' is the default mode) and nowhere\n                                 #   else\n\n# SCALARS\nprint('SCALARS\\n=======\\n')\n\nx = ad.new_scal(2)                   # Creates FADiff Scal x with value of 2\ny = ad.new_scal(5, name='y')         # 'name' parameter is optional\nf = x * y + ef.sin(x)                # Creates function f using Scal variables       \n                                     #   and FADiff's elemental sine operator\nprint(f'x.val --> {x.val}')          # Value of x\nprint(f'x.der --> {x.der}')          # Derivative of x \nprint(f'f.val --> {f.val}')          # Value of f\nprint(f'f.der --> {f.der}')          # Jacobian of f\nprint(f'f.der[0] --> {f.der[0]}')    # Partial derivative of f wrt x\nprint(f'f.der[1] --> {f.der[1]}')    # Partial derivative of f wrt y\n\nz = ad.new_scal(3)                   # Adding more variables\ng = x * y + ef.sin(x) + z\nprint(f'\\ng.val --> {g.val}')                    \nprint(f'g.der --> {g.der}')                     \n\n# VECTORS\nprint('\\nVECTORS\\n=======\\n')\n\nx1 = ad.new_vect([2, 3, 4])      # Creates FADiff Vect x1 with value [2, 3, 4]\nx2 = ad.new_vect([3, 2, 1])\nf = x1 - x2\nprint(f'x1.val --> {x1.val}')\nprint(f'x1.der --> {x1.der}')\nprint(f'f.val --> {f.val}')\nprint(f'f.der -->\\n{f.der}')\nprint(f'f.der[0] --> {f.der[0]}')\nprint(f'f.der[1] --> {f.der[1]}')\n\n# FUNCTION VECTOR w. SCALARS\nprint('\\nFUNCTION VECTOR w. SCALARS\\n==========================\\n')\n\nx = ad.new_scal(3)\ny = ad.new_scal(2)\nf1 = x * y + x\nf2 = 8 * y\nf = ad.new_funcvect([f1, f2])    # Creates FADiff FuncVect f with Scal f1 and f2\nprint(f'f.val --> {f.val}')\nprint(f'f.der -->\\n{f.der}')\n\n# FUNCTION VECTOR w. VECTORS\nprint('\\nFUNCTION VECTOR w. VECTORS\\n==========================\\n')\n\nx1 = ad.new_vect([2, 3, 4])\nx2 = ad.new_vect([1, 3, 2])\nf1 = x1 * x2\nf2 = x1 * 8\nf = ad.new_funcvect([f1, f2])    # Creates FADiff FuncVect f with Vect f1 and f2\nprint(f'f.val -->\\n{f.val}')\nprint(f'f.der -->\\n{f.der}')\n```\n\nTo run the above code, in a terminal window, navigate to the folder that contains the code file and run the following:\n\nFor Python 2 --\n```\npython code_file_name.py\n```\nFor Python 3 --\n```\npython3 code_file_name.py\n```\n\nwhere 'code_file_name' is the name of the user’s code file. The following output should then be rendered:\n\n```\nSCALARS\n=======\n\nx.val --> 2\nx.der --> 1\nf.val --> 10.909297426825681\nf.der --> [4.58385316 2.        ]\nf.der[0] --> 4.583853163452858\nf.der[1] --> 2.0\n\ng.val --> 13.909297426825681\ng.der --> [4.58385316 2.         1.        ]\n\nVECTORS\n=======\n\nx1.val --> [2 3 4]\nx1.der --> [1. 1. 1.]\nf.val --> [-1  1  3]\nf.der -->\n[[ 1.  1.  1.]\n [-1. -1. -1.]]\nf.der[0] --> [1. 1. 1.]\nf.der[1] --> [-1. -1. -1.]\n\nFUNCTION VECTOR w. SCALARS\n==========================\n\nf.val --> [ 9 16]\nf.der -->\n[[3 3]\n [0 8]]\n\nFUNCTION VECTOR w. VECTORS\n==========================\n\nf.val -->\n[[ 2  9  8]\n [16 24 32]]\nf.der -->\n[[[1. 3. 2.]\n  [2. 3. 4.]]\n\n [[8. 8. 8.]\n  [0. 0. 0.]]]\n```\n\nNote that the order of the partial derivatives in the Jacobian arrays are in the\nsame order as the order in which the user defined their corresponding FADiff objects\nin the user’s code file.\n","metadata":{"tags":[],"cell_id":"00015-7188557e-9278-4015-a2d7-c32a9734b9d9","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Test\nTo run our test code, in a terminal window clone our respository by running the following:\n\n```\ngit clone https://github.com/teamxvii/cs107-FinalProject.git\n```\n\nNavigate to the `cs107-FinalProject/FADiff` folder (where `test_main.py` is located) and run the following in the terminal:\n\n```\npytest\n```\n","metadata":{"cell_id":"00016-189a523d-6586-47c7-825b-bec625389f54","tags":[],"output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Software Organization\n\n### 1. What will the directory structure look like?\n\n```\ncs107-FinalProject/\n    FADiff/\n        FADiff.py\n        Elems.py\n        FuncVect.py\n        test_main.py\n        fad/\n            Gradients.py\n            Matrices.py\n        rev/\n            Gradients.py\n            Matrices.py\n    docs/\n        milestone1.ipynb\n        milestone2.ipynb\n        milestone2_progress.ipynb\n        documentation.ipynb\n    requirements.txt\n    ...\n```\n\n### 2. What modules do you plan on including? What is their basic functionality?\nOur FADiff package contains a module named `FADiff.py`. `FADiff.py` contains our main automatic differentiation class `FADiff()` that acts as our API. `Elems.py` contains elementary functions that are used to calculate the derivatives of\nall the elementary functions our package supports such as sine and cosine. `FuncVect.py` contains the class that represents vector functions. There are also a `Gradients.py` and a `Matrices.py` for both forward mode and reverse mode of our AD implementation located in the `fad` and `rev` folders respectively. Our package\nalso contains a module named `test_main.py` which contains our test class used in our\ntesting. We also used NumPy as an external dependency for our calculations. As explained in the\n“How to Use FADiff” section earlier in this document, our implementation\nlimits the use of external packages or only uses them internally\n(i.e., hidden from the API).\n\n### 3. Where will your test suite live? Will you use TravisCI? CodeCov?\nOur tests are in the `cs107-FinalProject/FADiff` folder in `test_main.py`.\nWe used TravisCI and CodeCov as well as pytest for testing. Please see the How to Use FADiff section earlier in this document for running tests.\n\n\n### 4. How will you distribute your package (e.g. PyPI)?\nWe successfully published our FADiff package on PyPI. User can use this command to install: <code> pip install FADiff==0.19</code> \n### 5. How will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n\nIn order to make our package installable using pip install, we packaged and uploaded to PyPI (Python Package Index). The reasons for choosing PyPI are: 1.PyPI is well-known and easy to use; 2.PyPI makes it easy to upload though sdist and wheels with the help of tutorial; 3. It provides a friendly and a broader environment for other developers to install our package, thus boosting stronger impact of our work. \nAfter adding the additional files(<code> \\_\\_init\\_\\_.py, setup.py, setup.cfg, LISCENSE.txt </code> ..) per PyPI requirements, the key structure of our final released package looks like this: \n```\nFADiff/\n    FADiff/\n        __init__.py\n        FADiff.py\n        FuncVect.py\n        fad/\n            __init__.py\n            Gradients.py\n            Matrices.py\n        rev/\n            __init__.py\n            Gradients.py\n            Matrices.py\n    requirements.txt\n    setup.py\n    setup.cfg\n    LISCENSE.txt\n    tests/\n    ...\n```\n    \nThen we leveraged setuptools, sdist and bdist_wheel to generate and upload the distribution archives. Finally, this framework worked very well! Moreover, since our source code is also clonable via Github, users can either <code>pip install FADiff==0.19 </code> or clone directly from GitHub repo.\n    \n### 6. Other considerations?\nAfter finishing Homework 4 and some upcoming lectures on various software topics such as containers, we may consider revising our software organization or change to another data structure (like graph) later on. We may also need to change the class / modules to make it flexible for other practical implementations like higher-order differentiation, Matrices differentiation, etc. For more on broader impact, ethical concerns and inclusivity statement, please consult the last section.","metadata":{"cell_id":"00015-f1214468-40c5-463f-a994-887828249efe","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Implementation\n\n### 1. Classes\nOur code relies on 6 main classes:\n* FADiff: API for package\n* funcVect: Vector function class for forward and reverse mode\n* fad/Scal: Scalar variable class for forward mode\n* fad/Vect: Vector variable class for forward mode\n* rev/Scal: Scalar variable class for reverse mode\n* rev/Vect: Vector variable class for reverse mode\n\n### 2. Class Attributes and Methods\nAttributes and methods for each class are described below:\n* FADiff: \n  - Contains global lists of all instances of forward mode scalar variables (fad/Scal), forward mode vector variables (fad/Vect), reverse mode scalar variables (rev/Scal), and reverse mode vector variables (rev/Vect)\n  - Contains methods for setting forward/reverse mode, creating a new scalar or vector object, and creating a new vector function\n* Scal (forward mode):\n  - Contains attributes for the value of a scalar variable, partial derivative of the variable, and parents of the variable, where \"parents\" are the original inputs (not intermediate values)\n  - Contains dunder methods for basic operations (addition, subtraction, multiplication, etc.), comparison operators (equals/not equals), and standard getters and setters\n* Vect (forward mode):\n  - Contains attributes for the value of a vector variable, partial derivative of the variable, and parents of the variable, where \"parents\" are the original inputs (not intermediate values)\n  - Contains dunder methods for basic operations (addition, subtraction, multiplication, etc.), comparison operators (equals/not equals), and standard getters and setters\n* Scal (reverse mode):\n  - Contains attributes for the value of a scalar variable and the partial derivative of the variable\n  - Contains dunder methods for basic operations (addition, subtraction, multiplication, etc.), comparison operators (equals/not equals), and standard getters and setters, all with a slightly different implementations than for forward mode to accomodate backpropogation mechanics\n* Vect (reverse mode):\n  - Contains attributes for the value of a scalar variable and the partial derivative of the variable\n  - Contains dunder methods for basic operations (addition, subtraction, multiplication, etc.), comparison operators (equals/not equals), and standard getters and setters, all with a slightly different implementations than for forward mode to accomodate backpropogation mechanics\n\n### 3. Data Structures\nValues for scalar variables are stored as floats or ints. Values for vector variables are stored in NumPy arrays. The partial derivative of a forward mode scalar variable is stored as a dictionary where the keys are all instances of the Scal (for scalar) class and the values are the partial derivative of the particular variable with respect to each key. The same dictionary structure is used for the partial derivative of forward mode vector variables, but the keys are all instances of the Vect (for vector) class and the values are partial derivatives in the form of a NumPy array. For reverse mode, scalar and vector variables, a dictionary is also used to store partial derivatives, but the values are lists of tuples containing the parent variable and the partial derivatives of the parent with respect to the key (an instance of the Scal/Vect class for reverse mode). Vector functions are stored as lists. Lastly, all instances of Scal or Vect objects are stored in lists of the AutoDiff class. \n\n### 4. Custom Elementary Functions\nThe `Elems.py` module contains custom functions for basic elementary operations using the classes described above.\n    \n### 5. External Dependencies\nWe rely on NumPy for array storage and mathematical operations on scalar and vector variables (trigonmetric functions, exponentiation, etc).\n","metadata":{"cell_id":"00016-6429e878-746a-43e7-b72e-9564285c6a7c","tags":[],"output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Final Extension\n\n### 1. Reflection of Milestone 2\n##### Future Features in Milestone 2\n\nThings to implement next\n\nIn this Milestone, we treat the Jacobian as a scalar and it can only handle the case of a single function of a single scalar input. Moving forward, we want to generalize the package for broader use cases.\n\n1. We will make the forward mode automatic differentiation object be able to access Jacobian Matrix.\n2. We will make the object capable of calculating partial derivatives. The challenging part is how to handle the number of variables we have, e.g if we define a class to calculate derivatives for multivariable functions f(x,y,z) and f(x,y,z,m,n), which data structure should we use as the attribute of the class? If we use an array instead of a scalar as the attribute, how can we implement the differentiation in an array?\n3. We might also think about how to calculate differentiation for polynomial functions like f(x) = x + sin(x) + cos(x). The challenging part here is how to implement a dunder method to handle the order of add or substraction. Presumably, we might need to change the classes or change the data structures or add new modules. We should consult TAs and Prof. Sondak for more instruction and insights on implementation.\n\n##### Feedback from Milestone 2 [3pts] (D Sondak) 1 pt\nThe future feature should be presented in a clearer manner including concrete steps on how you will realize the implementation of this extension. You are already required to implement the full version of forward mode (multiple functions and multiple inputs) for this project. The example provided (f(x) = x + sin(x) + cos(x)) should already work with the current implementation.\n\nPlease set up a meeting with Tosin to discuss potential extensions as soon as possible.\n\n##### Reflection\nIn Milestone2, we misunderstood the requirement for future features as what we would do toward the minimum requirement on Forward Mode for the final deliverable. Then we communicated with our TA and instructor and clarified the expectations and the right direction. Our extension is implementing the reverse mode and allowing users the flexibility of choosing and switching between forward and reverse mode. \n\n### 2. Description of extension feature\n1. We implemented reverse mode which realizes the same functionality as forward mode in a separate module.\n2. We also created an efficient and easy-to-use setting to allow users to switch between modes: The default setting is forward mode, but if one wants to use reverse mode, they can just put `FADiff.set_mode('reverse')` at the top of the code file to call our API functions for reverse mode. If they want to switch back to forward mode, they can simply use `FADiff.set_mode('forward')`. \n  - Note: We do NOT recommend switching between forward and reverse mode within a file. Users should choose one method for the duration of their analysis.\n\n### 3. Background of extension -- reverse mode\nIn reverse mode, the algorithm first goes through the forward evaluation trace (see example in the Background Section 2.4) to get the values of the intermediate nodes, and then reverses backwards through the trace to get the gradients. In calculating the gradients, it starts with the final outcome, initializes its derivative (with respect to itself) to 1, and then reversely computes the precursive variable's derivative with respect to its inputs until it reaches the root variable.\n<br>\n<br> In mathemetical form: \n- If $z_1,,,z_m$ are scalar inputs:\n$$z_1 = f_1(z_0)$$\n$$z_2 = f_2(z_1)$$\n$$...$$\n$$f = z_m = f_m(z_{m-1})$$\nThen the derivative of $f$ with respect to $z_0$ :\n\\begin{equation}\n\\begin{split}\n\\frac{dz_m}{dz_0}& = \\frac{dz_m}{dz_{m-1}}\\frac{dz_{m-1}}{dz_{0}}\\\\\n& = \\frac{dz_m}{dz_{m-1}} \\frac{dz_{m-1}}{dz_{m-2}}\\frac{dz_{m-2}}{dz_0}\\\\\n&...\\\\\n&=\\frac{dz_m}{dz_{m-1}} \\frac{dz_{m-1}}{dz_{m-2}}...\\frac{dz_{1}}{dz_0}\n\\end{split}\n\\end{equation}\n\n- If the input or intermediate variable is a vector:\n\\begin{equation}\n\\begin{split}\nu_1 & = f(v)\\\\\nu_2 & = g(v)\\\\\nt & = h(u_1,u_2)\\\\\n\\frac{d_t}{d_v} & = \\sum_{i}\\frac{dt}{du_i} \\frac{du_i}{dv}\n\\end{split}\n\\end{equation}\n\n","metadata":{"cell_id":"00016-992c141d-2dc9-4765-8dd6-3e4610e49a06","tags":[],"output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Broader Impact and Inclusivity Statement \n### Broader Impact\n\n1. On a positive side, our software gives users a flexible, convenient problem-solving tool for automatically calculating derivatives in multiple settings without coding each intermediate step on their own or iteratively calculating the necessary steps by hand. This convenience is especially important for large-scale computation and complex networks. For example, consider a 20-layer neural network with 100 nodes in each layer. Hand-coding the derivatives and back propogation steps is time-consuming and easily susceptible to bugs and errors. However, by using our package, users simply define their activation function and loss function, and the software will do all the necessary operations automatically in a few seconds with respect to every parameter. Moreover, our work also makes it straightforward to implement a gradient-based learning algorithm like gradient descent optimization, which is particularly impactful in both scientific research and real-word applications.\n\n\n2. However, there can also be some situations when people misuse our software. First, it only supports the minimum requirements and cannot support the case of multiple vector inputs with a vector function. Second, it only provides the first derivative. We must highlight these limitations in the documentation in case a user implements our software in an unsuitable setting, thus causing errors.\n\n\n3. We are also taking the risk of passivating human intelligence for young students. Consider a young student who is studying calculus and derivatives or machine learning theories. Presumably, it is good for them to learn every detail of mathematical methods with hands-on experience. If they use our software in assignments, interviews, or exams, they may achieve accurate answers but lack a deeper understanding of the specifics of the calculations. Although using software is fast and efficiency is a high priority in many cases, it is harmful for a beginner to take short cuts. This software could encourage a young student to omit helpful (albeit repetitive) programming exercises.\n\n\n4. Another ethical consideration is that we risk causing negative downstream effects. We have no knowledge of our users' exact purpose in taking advantage of our software, which could harbor illegal or potentially perilous implications for others. If this is the case, then creating a product that might cause harmful downstream effects to society is very much an ethical concern.\n\n### Inclusivity Statement\n\nIt has been a hot issue that computer science and software development  should encourage diversity and be more inclusive to minority groups, for example, women, under-represented groups, people with disability, etc.\nOur software should also serve as a practice to pomote diversity, equity and inclusion and enable a equal contribution to our code base for all groups and individuals. \n\n\n1. The first thing we would like to advocate for is to make our software an open-source resource to everyone, enabling people of all backgrounds to create pull requests, add comments to the development, and make conversations public. \n\n\n2. Second, we would also like to request that users not disclose their identity like gender/race/education level/institutions when posting, raising errors, or creating pull requests on the repo. In this way, we can encourage more contributions from minority groups (e.g., under-represented students) without biasing individuals or creating an atmosphere of only accepting contributions from \"professional masters\". If possible, we also hope to add multi-language translated documentation for users in non-English-speaking parts of the world in order to empower more diverse participants.\n\n\n3. Finally, we would also like to provide guidance and resources to engage economically disadvantaged students and people with disabilities. We plan on always placing a diversity and inclusivity statement in our documentation and/or software page declaring that we actively encourage contributions from people of any age, culture, ethnicity, gender, geographic location, education, ability, race and socio-economic background.","metadata":{"cell_id":"00020-4d9f729a-c64d-4cc6-a595-98c26b1e405c","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Future Work\n\n### What we want to add\nOur package only calculates the first-order derivative, but in the future, we want to add higher-order derivatives. \n\n### Why\nFirst-order derivatives can only tell us the direction of movement. But the higher order derivatives can enable more use cases, for example, the second derivative can tell the rate of increase or decrease; the third order derivative can be useful in analyzing the jerk or jolt over time, an important quantity in engineering and motion control. From a statistical prospective, higher-order derivatives enables Riemannian Hamiltonian Monte Carlo sampler to overcome severe analytically-difficult problems (such as Bayesian Hierachical model or latent model) that first-order sampling cannot by using the second and third order derivatives. (Charles C. Margossian, 2013) Higher-order derivatives can also help to find the approximated solutions to higher-order equations in modeling dispersion in classical and quantum mechanics and chemical structures (Xingfang Jiang, 2011).\n\n\n### How to implement\nLet's take second-order derivatives as an example: One option is to add a feature to allow our current package to return the symbolic function of the first-order derivative, based on which, we can apply the first-order automatic differentiation twice. Applying the forward mode twice can get us a Hessian matrix ($H = f^{''}$) (or Hessian vector product), which we could alternatively calculate with a truncated Taylor series in a particular direction to get a higher-order derivative of scalar inputs.","metadata":{"cell_id":"00021-94c1f291-2f0f-45d6-b363-d41e5433e523","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00022-86773821-e4b4-4b6e-911c-470204fdc177","output_cleared":false,"source_hash":"b623e53d","execution_start":1607785629879,"execution_millis":2,"deepnote_cell_type":"code"},"source":"","execution_count":0,"outputs":[]}],"nbformat":4,"nbformat_minor":4,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"53cfc77d-cb46-4fc0-893b-180cbb28d26a","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}}}